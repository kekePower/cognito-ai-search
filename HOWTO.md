# Cognito AI Search v1.1.0 - Technical Documentation

This document contains detailed technical information about setting up and configuring Cognito AI Search v1.0.x. For a general overview, please see the [README.md](README.md).

## Table of Contents
- [Prerequisites](#prerequisites)
- [Environment Setup](#environment-setup)
- [SearXNG Configuration](#searxng-configuration)
- [Ollama Setup](#ollama-setup)
- [Development](#development)
- [Production Deployment](#production-deployment)
- [Docker Deployment](#docker-deployment)
- [IPv6 Support Configuration](#ipv6-support-configuration)
- [New Features in v1.0.0](#new-features-in-v100)
- [Troubleshooting](#troubleshooting)

## Prerequisites

- Node.js 18+ and npm/yarn/pnpm
- Self-hosted SearXNG instance (or public instance URL)
- Ollama installed and running with desired models
- (Optional) Redis for caching (recommended for SearXNG)

## Environment Setup

Create a `.env.local` file in the root directory with the following variables:

```env
# SearXNG Configuration
SEARXNG_API_URL=http://your-searxng-instance:port

# Ollama Configuration  
OLLAMA_API_URL=http://your-ollama-instance:11434
DEFAULT_OLLAMA_MODEL=phi4-mini:3.8b-q8_0
AI_RESPONSE_MAX_TOKENS=100

# Next.js Configuration
NEXT_PUBLIC_APP_URL=http://localhost:3000
```

### Recommended Model Configuration

For optimal performance in v1.0.0, we recommend:
- **Model**: `phi4-mini:3.8b-q8_0` (fast and efficient)
- **Max Tokens**: `100` (quick responses)
- **Alternative**: `mistral` or `llama3` for more detailed responses

## SearXNG Configuration

1. Install and configure a SearXNG instance following the [official documentation](https://docs.searxng.org/admin/installation.html)
2. Enable API and JSON support by modifying the following settings in `searx/settings.yml`:

   ```yaml
   search:
     api_enabled: true
     formats:
       - html
       - json
       - csv
       - rss
   
   server:
     bind_address: "0.0.0.0"  # Allow external connections
     secret_key: "your-secure-secret-key"  # Generate a secure key
     http_protocol_version: "1.1"
   ```

3. Update Redis socket path in `manage` script if needed:
   ```bash
   _dev_redis_sock="/run/redis/redis.sock"
   ```

4. Disable automatic browser opening in development (optional):
   ```bash
   # In manage script
   SEARXNG_DEBUG=0 pyenv.cmd python -m searx.webapp
   ```

## Ollama Setup

1. Install Ollama from [ollama.ai](https://ollama.ai/)
2. Pull your preferred model:
   ```bash
   ollama pull mistral
   # or
   ollama pull llama3
   ```
3. Start the Ollama server

## Development

```bash
# Install dependencies
pnpm install

# Start development server
pnpm dev

# Build for production
pnpm build

# Run production server
pnpm start

# Lint code
pnpm lint

# Type checking
pnpm type-check
```

## Production Deployment

### Building for Production

1. Ensure all dependencies are installed:
   ```bash
   pnpm install
   ```

2. Create a production build:
   ```bash
   pnpm build
   ```
   This will create an optimized production build in the `.next` directory.

### Running in Production

1. Start the production server:
   ```bash
   pnpm start
   ```
   The application will be available at `http://localhost:3000` by default.

### Environment Configuration for Production

For production, make sure to set the following environment variables:

```env
NODE_ENV=production
NEXT_TELEMETRY_DISABLED=1

# Update these with your production URLs
SEARXNG_API_URL=your-production-searxng-url
OLLAMA_API_URL=your-production-ollama-url
DEFAULT_OLLAMA_MODEL=your-preferred-model
AI_RESPONSE_MAX_TOKENS=your-preferred-max-tokens
NEXT_PUBLIC_APP_URL=your-production-url
```

### Using PM2 (Recommended for Production)

For better process management in production, you can use PM2:

1. Install PM2 globally:
   ```bash
   pnpm add -g pm2
   ```

2. Start the application with PM2:
   ```bash
   pm2 start "pnpm start" --name "cognito-ai-search"
   ```

3. Set PM2 to start on system boot:
   ```bash
   pm2 startup
   pm2 save
   ```

4. Monitor your application:
   ```bash
   pm2 monit
   ```

## Docker Deployment

You can deploy Cognito AI Search using Docker in two ways: using our pre-built Docker Hub image (recommended) or building your own image. This provides a containerized environment for easier setup and management, especially when integrating with other services like SearXNG using Docker Compose.

### Prerequisites

- Docker installed on your system.

### Option 1: Using Pre-built Docker Hub Image (Recommended)

The easiest way to get started is using our official Docker Hub image:

```bash
docker run -d \
  -p 3000:3000 \
  -e OLLAMA_API_URL="http://YOUR_OLLAMA_HOST:11434" \
  -e DEFAULT_OLLAMA_MODEL="phi4-mini:3.8b-q8_0" \
  -e AI_RESPONSE_MAX_TOKENS="1200" \
  -e SEARXNG_API_URL="http://YOUR_SEARXNG_HOST:8888" \
  --name cognito-ai-search \
  kekepower/cognito-ai-search:latest
```

**Available Docker Hub Tags:**
- `kekepower/cognito-ai-search:latest` - Latest stable release
- `kekepower/cognito-ai-search:1.1.0` - Specific version tags

**Environment Variables:**
- `OLLAMA_API_URL`: URL for your Ollama instance
- `SEARXNG_API_URL`: URL for your SearXNG instance  
- `DEFAULT_OLLAMA_MODEL`: The default Ollama model to use (recommended: `phi4-mini:3.8b-q8_0`)
- `AI_RESPONSE_MAX_TOKENS`: Maximum tokens for AI responses (recommended: 1200)
- `PORT`: The port the application listens on (default: 3000)

The application will be available at `http://localhost:3000`.

### Option 2: Building Your Own Docker Image

If you prefer to build the image yourself or need to customize the build:

1.  Navigate to the root directory of the Cognito AI Search project (where the `Dockerfile` is located).
2.  Build the Docker image using the following command:

    ```bash
    docker build -t cognito-ai-search .
    ```

    **Important for Client-Side Configuration:**
    The application uses `NEXT_PUBLIC_` environment variables (e.g., `NEXT_PUBLIC_OLLAMA_API_URL`, `NEXT_PUBLIC_SEARXNG_API_URL`) that are embedded into the client-side code during the build process. If your client-side JavaScript needs to know these URLs (for instance, if you were making API calls directly from the browser to these services, though typically this app proxies through its own backend), you **must** provide them as build arguments. If these are not provided, their `NEXT_PUBLIC_` versions will be undefined in the client bundle.

    Example with build arguments:
    ```bash
    docker build \
      --build-arg OLLAMA_API_URL="http://your-ollama-host:11434" \
      --build-arg SEARXNG_API_URL="http://your-searxng-host:8080" \
      --build-arg DEFAULT_OLLAMA_MODEL="your-default-model" \
      -t cognito-ai-search .
    ```
    *Note: For typical deployments where Cognito AI Search backend handles communication with Ollama and SearXNG, these `NEXT_PUBLIC_` build arguments might not be strictly necessary if the client only talks to the Cognito AI Search backend. The runtime environment variables (see below) are generally more critical for server-side functionality.*

#### Running Your Custom Built Container

Once the image is built, you can run it as a container:

```bash
docker run -d -p 3000:3000 \
  -e PORT="3000" \
  -e OLLAMA_API_URL="http://host.docker.internal:11434" \
  -e SEARXNG_API_URL="http://host.docker.internal:8080" \
  -e DEFAULT_OLLAMA_MODEL="phi4-mini:3.8b-q8_0" \
  -e AI_RESPONSE_MAX_TOKENS=1200 \
  -e NODE_ENV=production \
  -e PORT=3000 \
  cognito-ai-search
```

-   `-d`: Run the container in detached mode (in the background).
-   `-p 3000:3000`: Maps port 3000 on your host to port 3000 in the container.
-   `-e VARIABLE_NAME="value"`: Sets runtime environment variables for the application.
    -   `PORT`: The port the Next.js application will listen on inside the container.
    -   `OLLAMA_API_URL`: URL for your Ollama instance. `host.docker.internal` is a special DNS name that resolves to your host machine's IP address from within the container (useful if Ollama is running directly on your host).
    -   `SEARXNG_API_URL`: URL for your SearXNG instance.
    -   `DEFAULT_OLLAMA_MODEL`: The default Ollama model to use.
-   `--name cognito-ai-search-container`: Assigns a name to the running container.

Adjust the environment variables as needed for your setup.

### Using Docker Compose with SearXNG

For a more integrated setup, especially with SearXNG, you can use `docker-compose`. Here's an example `docker-compose.yml` using the Docker Hub image:

```yaml
version: '3.8'

services:
  cognito-ai-search:
    image: kekepower/cognito-ai-search:latest
    container_name: cognito-ai-search
    ports:
      - "3000:3000"
    environment:
      - PORT=3000
      - OLLAMA_API_URL=http://host.docker.internal:11434
      - SEARXNG_API_URL=http://searxng:8080
      - DEFAULT_OLLAMA_MODEL=phi4-mini:3.8b-q8_0
      - AI_RESPONSE_MAX_TOKENS=1200
      - NODE_ENV=production
    depends_on:
      - searxng
    networks:
      - cognito_network

  searxng:
    image: searxng/searxng:latest
    container_name: searxng
    ports:
      - "8080:8080"
    volumes:
      - ./searxng:/etc/searxng
    environment:
      - SEARXNG_BASE_URL=http://localhost:8080/
    networks:
      - cognito_network

networks:
  cognito_network:
    driver: bridge
```

To run with Docker Compose:

```bash
docker-compose up -d
```

This Docker setup provides a flexible way to deploy and manage Cognito AI Search. The Docker Hub image is automatically updated with each new release through our CI/CD pipeline.

## IPv6 Support Configuration

Cognito AI Search can be configured to be accessible over IPv6. This requires changes at the application, Docker host, and deployment levels.

### 1. Application Configuration (`package.json`)

The `scripts.start` command in `package.json` has been updated to:

```json
"start": "next start -H ::"
```

This tells the Next.js application to listen on all available IPv6 (and by default, IPv4-mapped IPv6) interfaces within its container.

### 2. Docker Host Configuration

Your Docker daemon on the host machine must be configured to enable IPv6. This is a one-time setup on the machine running Docker.

1.  **Edit `daemon.json`**:
    *   Locate or create the Docker daemon configuration file (usually at `/etc/docker/daemon.json`).
    *   Add or ensure the following content is present:
        ```json
        {
          "ipv6": true,
          "fixed-cidr-v6": "2001:db8:1::/64" 
        }
        ```
        *Note: You can replace `"2001:db8:1::/64"` with a different unique local IPv6 unicast address (ULA) block if this one is already in use on your network or if you have a specific range assigned.*

2.  **Restart Docker Service**:
    *   After saving `daemon.json`, restart the Docker service. The command varies by system:
        *   `sudo systemctl restart docker` (most common on Linux)
        *   `sudo service docker restart` (older systems)

### 3. Deployment Configuration

#### Using `docker-compose.yml` (Recommended)

The provided `docker-compose.yml` file is pre-configured for IPv6:

*   **Port Mapping**: The `cognito-ai-search` service uses `ports: - "[::]:${APP_PORT:-3000}:3000"`. This maps port 3000 of the container to the specified host port (defaulting to 3000) on all IPv6 and IPv4 interfaces of the host.
*   **Network**: An IPv6-enabled Docker network (`cognito_network`) is defined and used by the service:
    ```yaml
    networks:
      cognito_network:
        driver: bridge
        enable_ipv6: true
        ipam:
          driver: default
          config:
            - subnet: "fd00:db8:cognito::/64" # Example IPv6 ULA subnet
    ```

To run with Docker Compose:

```bash
docker-compose up -d --build
```

#### Using `docker run` (Manual)

If you are not using Docker Compose, you need to modify your `docker run` command to map the port to IPv6 on the host:

```bash
docker run -d --name cognito-ai-search \
  -p [::]:3000:3000 \
  -e OLLAMA_API_URL="http://[your_ollama_host_or_ipv6]:11434" \
  -e SEARXNG_API_URL="http://[your_searxng_host_or_ipv6]:8080" \
  -e AI_RESPONSE_MAX_TOKENS=1200 \
  -e DEFAULT_OLLAMA_MODEL="phi4-mini:3.8b-q8_0" \
  -e NODE_ENV=production \
  -e PORT=3000 \
  cognito-ai-search # Your built image name
```

*Ensure you replace placeholder URLs and add all required environment variables.*

### 4. Accessing the Service via IPv6

Once configured and running, you can access Cognito AI Search using the IPv6 address of your host machine:

`http://[<ipv6_address_of_host>]:3000/`

Replace `<ipv6_address_of_host>` with the actual IPv6 address.

## New Features in v1.0.x

### üé® Modern UI/UX
- **Dark/Light Theme**: Automatic theme detection with manual toggle
- **Gradient Animations**: Beautiful input field animations (single run, not looping)
- **Glass Morphism**: Modern backdrop blur effects throughout the interface
- **Responsive Design**: Optimized for all device sizes

### üöÄ Enhanced Performance
- **Smart Caching**: Search results cached locally for 24 hours
- **Query Optimization**: AI-powered search query enhancement
- **Async Loading**: Search results display immediately, AI responses load in background
- **Health Checks**: Automatic service availability verification

### üîç Intelligent Search Features
- **Search Suggestions**: Dynamic suggestions with refresh capability
- **Recent Searches**: Persistent search history with individual removal
- **Optimized Queries**: AI enhances search terms for better results
- **Fallback Mechanisms**: Graceful degradation when services are unavailable

### üõ†Ô∏è Technical Improvements
- **Custom Hooks**: `useSearch` and `useSearchSuggestions` for state management
- **Modular API**: Separated concerns with dedicated API utilities
- **TypeScript**: Full type safety throughout the application
- **Error Handling**: Comprehensive error management with user-friendly messages

### üì± User Experience
- **Loading States**: Clear visual feedback during operations
- **Smooth Transitions**: Polished animations and micro-interactions
- **Accessibility**: Improved keyboard navigation and screen reader support
- **Mobile Optimized**: Touch-friendly interface with responsive design

## Troubleshooting

### Common Issues

1. **SearXNG API not accessible**
   - Ensure SearXNG is running and accessible
   - Verify the API is enabled in SearXNG settings
   - Check CORS settings if accessing from a different domain

2. **Ollama connection issues**
   - Make sure Ollama server is running
   - Verify the API URL and port in your environment variables
   - Check if the specified model is downloaded

3. **Build errors**
   - Ensure all dependencies are installed
   - Check Node.js version compatibility
   - Clear Next.js cache if needed: `rm -rf .next`

For additional help, please [open an issue](https://github.com/kekePower/cognito-ai-search/issues).
